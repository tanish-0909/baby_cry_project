{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Baby Face from CCTV footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install ultralytics moviepy opencv-python torch torchvision torchaudio\n",
    "\n",
    "# Add parent folder to sys.path and import config\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import config as cfg\n",
    "\n",
    "# Expose commonly used variables with safe fallbacks\n",
    "DATA_ROOT = getattr(cfg, 'DATA_ROOT', \"/kaggle/input/listening-beyond-the-cry/Final Dataset\")\n",
    "DATA_DIR = getattr(cfg, 'DATA_DIR', DATA_ROOT)\n",
    "MODEL_PATH = getattr(cfg, 'CRY_TYPE_WEIGHTS', getattr(cfg, 'CRY_TYPE_WEIGHTS', 'best_infantcry_model.pth'))\n",
    "TEST_AUDIO = getattr(cfg, 'TEST_AUDIO', None)\n",
    "INPUT_VIDEO1 = getattr(cfg, 'INPUT_VIDEO1', None)\n",
    "YOLO_WEIGHTS = getattr(cfg, 'YOLO_WEIGHTS', None)\n",
    "\n",
    "print('Config loaded from parent folder:', os.path.abspath(os.path.join('..','config.py')))\n",
    "print('DATA_ROOT:', DATA_ROOT)\n",
    "print('MODEL_PATH:', MODEL_PATH)\n",
    "print('INPUT_VIDEO1:', INPUT_VIDEO1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install roboflow ultralytics supervision moviepy\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"tmlb5en1LBu6OosgU9FL\")\n",
    "project = rf.workspace(\"sleepingbaby\").project(\"baby-project\")\n",
    "version = project.version(1)\n",
    "\n",
    "dataset = version.download(\"yolov11\", location=\"roboflow_dataset\")\n",
    "print(\"Dataset downloaded to:\", dataset.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def filter_labels(folder, keep_classes=[0]):\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        lbl_dir = os.path.join(folder, split, \"labels\")\n",
    "        if not os.path.exists(lbl_dir):\n",
    "            print(\"WARNING: missing labels folder â†’\", lbl_dir)\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(lbl_dir):\n",
    "            if not file.endswith(\".txt\"):\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(lbl_dir, file)\n",
    "            with open(path, \"r\") as f:\n",
    "                lines = f.read().strip().splitlines()\n",
    "\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "\n",
    "                parts = line.split()\n",
    "                if len(parts) < 5:\n",
    "                    continue  # skip invalid YOLO lines\n",
    "\n",
    "                try:\n",
    "                    cls = int(parts[0])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if cls in keep_classes:\n",
    "                    parts[0] = \"0\"  # map to single class\n",
    "                    new_lines.append(\" \".join(parts))\n",
    "\n",
    "            # Write updated labels back\n",
    "            with open(path, \"w\") as f:\n",
    "                if new_lines:\n",
    "                    f.write(\"\\n\".join(new_lines))\n",
    "\n",
    "print(\"Filtering labels...\")\n",
    "filter_labels(\"roboflow_dataset\", keep_classes=[0])\n",
    "print(\"Done: Kept only class 0 (face).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_yaml = \"\"\"\\\n",
    "path: roboflow_dataset\n",
    "\n",
    "train: train/images\n",
    "val: valid/images\n",
    "test: test/images\n",
    "\n",
    "nc: 1\n",
    "names:\n",
    "  0: face\n",
    "\"\"\"\n",
    "with open(\"data.yaml\", \"w\") as f:\n",
    "    f.write(data_yaml)\n",
    "\n",
    "print(\"Correct data.yaml written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "root = \"roboflow_dataset\"\n",
    "bad_files = []\n",
    "\n",
    "for sub in [\"train\", \"valid\", \"test\"]:\n",
    "    img_dir = os.path.join(root, sub, \"images\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(\"WARNING: missing images folder â†’\", img_dir)\n",
    "        continue\n",
    "\n",
    "    for f in os.listdir(img_dir):\n",
    "        path = os.path.join(img_dir, f)\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            bad_files.append(path)\n",
    "            print(\"CORRUPT:\", path)\n",
    "\n",
    "print(\"\\nTotal corrupt files:\", len(bad_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root = \"roboflow_dataset/train/images\"\n",
    "bad = []\n",
    "zero = []\n",
    "\n",
    "for f in os.listdir(root):\n",
    "    path = os.path.join(root, f)\n",
    "    size = os.path.getsize(path)\n",
    "    if size == 0:\n",
    "        zero.append(path)\n",
    "    if size < 500:  # suspiciously tiny\n",
    "        bad.append((path, size))\n",
    "\n",
    "len(zero), len(bad), zero[:5], bad[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "root = \"roboflow_dataset\"\n",
    "\n",
    "valid_ext = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "bad_files = []\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    img_dir = os.path.join(root, split, \"images\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(\"Missing:\", img_dir)\n",
    "        continue\n",
    "\n",
    "    for f in os.listdir(img_dir):\n",
    "        path = os.path.join(img_dir, f)\n",
    "\n",
    "        # Check extension\n",
    "        ext = os.path.splitext(f)[1].lower()\n",
    "        if ext not in valid_ext:\n",
    "            print(\"âš  Removing non-image:\", path)\n",
    "            os.remove(path)\n",
    "            continue\n",
    "\n",
    "        # Check readable image\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(\"âŒ Removing corrupt image:\", path)\n",
    "            os.remove(path)\n",
    "            bad_files.append(path)\n",
    "\n",
    "print(\"\\nRemoved corrupt files:\", len(bad_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root = \"roboflow_dataset\"\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    lbl_dir = os.path.join(root, split, \"labels\")\n",
    "    if not os.path.exists(lbl_dir):\n",
    "        continue\n",
    "\n",
    "    for f in os.listdir(lbl_dir):\n",
    "        path = os.path.join(lbl_dir, f)\n",
    "        if os.path.getsize(path) == 0:\n",
    "            print(\"âš  Removing empty label:\", path)\n",
    "            os.remove(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def is_image(path):\n",
    "    try:\n",
    "        img = cv2.imread(path)\n",
    "        return img is not None\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "root = \"roboflow_dataset\"\n",
    "\n",
    "bad_files = []\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    folder = os.path.join(root, split)\n",
    "    for fname in os.listdir(folder):\n",
    "        path = os.path.join(folder, fname)\n",
    "\n",
    "        # skip labels directory\n",
    "        if os.path.isdir(path):\n",
    "            continue\n",
    "\n",
    "        # check for valid image extensions\n",
    "        if not fname.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
    "            bad_files.append(path)\n",
    "            os.remove(path)\n",
    "            continue\n",
    "\n",
    "        # check if image opens properly\n",
    "        if not is_image(path):\n",
    "            bad_files.append(path)\n",
    "            os.remove(path)\n",
    "\n",
    "print(\"Removed bad files:\")\n",
    "for f in bad_files:\n",
    "    print(f)\n",
    "\n",
    "print(\"Cleaning done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root = \"roboflow_dataset\"\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    labels = os.path.join(root, split, \"labels\")\n",
    "    for fname in os.listdir(labels):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            print(\"âš ï¸ Removing non-txt file inside labels:\", fname)\n",
    "            os.remove(os.path.join(labels, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "ROOT = \"/kaggle/working/roboflow_dataset\"\n",
    "\n",
    "def safe_read(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        arr = np.frombuffer(data, np.uint8)\n",
    "        img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Exception while reading\", path, \":\", e)\n",
    "        return None\n",
    "\n",
    "bad_files = []\n",
    "\n",
    "print(\"ðŸ” Scanning ALL image files YOLO may attempt to load...\\n\")\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    img_dir = os.path.join(ROOT, split, \"images\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(\"MISSING FOLDER:\", img_dir)\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(img_dir):\n",
    "        full = os.path.join(img_dir, file)\n",
    "        if not file.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
    "            continue\n",
    "\n",
    "        img = safe_read(full)\n",
    "        if img is None:\n",
    "            print(\"âŒ CORRUPTED IMAGE:\", full)\n",
    "            bad_files.append(full)\n",
    "        else:\n",
    "            h, w = img.shape[:2]\n",
    "            if h == 0 or w == 0:\n",
    "                print(\"âŒ ZERO-SIZE IMAGE:\", full)\n",
    "                bad_files.append(full)\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"FINISHED SCANNING\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "if bad_files:\n",
    "    print(\"âŒ BAD FILES FOUND:\")\n",
    "    for b in bad_files:\n",
    "        print(\"â†’\", b)\n",
    "else:\n",
    "    print(\"âœ… No corrupted images found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(bad_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"Loading YOLO model...\")\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.train(\n",
    "    data=\"data.yaml\",\n",
    "    epochs=40,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=0,    # GPU on Kaggle\n",
    "    amp=False    # STABLE FIX FOR KAGGLE AMP BUG\n",
    ")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics.utils.plotting import plot_results\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"RESULTS\")\n",
    "\n",
    "#combined results\n",
    "results_dir = model.results_dir\n",
    "if results_dir is None:  # fallback for older versions\n",
    "    results_dir = \"runs/detect/train\"\n",
    "\n",
    "train_results_file = os.path.join(results_dir, \"results.png\")\n",
    "\n",
    "if os.path.exists(train_results_file):\n",
    "    print(\"\\nTraining Curves:\")\n",
    "    display(Image(filename=train_results_file))\n",
    "else:\n",
    "    print(\"results.png not found, manually plotting...\")\n",
    "    try:\n",
    "        plot_results(file=os.path.join(results_dir, \"results.csv\"))\n",
    "        display(Image(filename=train_results_file))\n",
    "    except:\n",
    "        print(\"Could not generate training curves.\")\n",
    "\n",
    "#last validation batch predictions\n",
    "val_batch_img = os.path.join(results_dir, \"val_batch0_pred.jpg\")\n",
    "if os.path.exists(val_batch_img):\n",
    "    print(\"\\nValidation Predictions (val_batch0_pred.jpg):\")\n",
    "    display(Image(filename=val_batch_img))\n",
    "else:\n",
    "    print(\"val_batch0_pred.jpg not found.\")\n",
    "\n",
    "#validation labels vs predictions\n",
    "val_labels_img = os.path.join(results_dir, \"val_batch0_labels.jpg\")\n",
    "if os.path.exists(val_labels_img):\n",
    "    print(\"\\nValidation Labels:\")\n",
    "    display(Image(filename=val_labels_img))\n",
    "else:\n",
    "    print(\"val_batch0_labels.jpg not found.\")\n",
    "\n",
    "\n",
    "#PR curve\n",
    "pr_curve_file = os.path.join(results_dir, \"PR_curve.png\")\n",
    "if os.path.exists(pr_curve_file):\n",
    "    print(\"\\nPrecision-Recall Curve:\")\n",
    "    display(Image(filename=pr_curve_file))\n",
    "else:\n",
    "    print(\"PR_curve.png not found.\")\n",
    "\n",
    "#F1 curve\n",
    "f1_curve_file = os.path.join(results_dir, \"F1_curve.png\")\n",
    "if os.path.exists(f1_curve_file):\n",
    "    print(\"\\nF1 Curve:\")\n",
    "    display(Image(filename=f1_curve_file))\n",
    "else:\n",
    "    print(\"F1_curve.png not found.\")\n",
    "\n",
    "#P curve\n",
    "p_curve_file = os.path.join(results_dir, \"P_curve.png\")\n",
    "if os.path.exists(p_curve_file):\n",
    "    print(\"\\nPrecision Curve:\")\n",
    "    display(Image(filename=p_curve_file))\n",
    "else:\n",
    "    print(\"P_curve.png not found.\")\n",
    "\n",
    "#SHOW R curve\n",
    "r_curve_file = os.path.join(results_dir, \"R_curve.png\")\n",
    "if os.path.exists(r_curve_file):\n",
    "    print(\"\\nRecall Curve:\")\n",
    "    display(Image(filename=r_curve_file))\n",
    "else:\n",
    "    print(\"R_curve.png not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TESTING: process a sample video using config paths\n",
    "import cv2\n",
    "import moviepy.editor as mp\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "input_video = INPUT_VIDEO1 \n",
    "wav_output = \"audio.wav\"           # extracted audio file\n",
    "silent_video = \"video_no_audio.mp4\"\n",
    "final_output = \"output_with_box.mp4\"\n",
    "\n",
    "print(\"Extracting audio...\")\n",
    "clip = mp.VideoFileClip(input_video)\n",
    "clip.audio.write_audiofile(wav_output)\n",
    "print(\"Saved:\", wav_output)\n",
    "\n",
    "print(\"Removing audio...\")\n",
    "clip_no_audio = clip.without_audio()\n",
    "clip_no_audio.write_videofile(silent_video, codec='libx264', audio=False)\n",
    "print(\"Saved:\", silent_video)\n",
    "\n",
    "# use YOLO weights from config if available\n",
    "yolo_model_path = YOLO_WEIGHTS if YOLO_WEIGHTS else \"yolo11n.pt\"\n",
    "print('Using YOLO model:', yolo_model_path)\n",
    "model = YOLO(yolo_model_path)\n",
    "\n",
    "cap = cv2.VideoCapture(silent_video)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(final_output, fourcc, fps, (width, height))\n",
    "\n",
    "print(\"Processing video...\")\n",
    "\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLO prediction\n",
    "    results = model(frame, conf=0.5, verbose=False)\n",
    "\n",
    "    # Draw boxes\n",
    "    for r in results:\n",
    "        boxes = getattr(r.boxes, 'xyxy', None)\n",
    "        if boxes is None:\n",
    "            continue\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box[:4])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 50 == 0:\n",
    "        print(\"Processed frame:\", frame_idx)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Saved final video:\", final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"700\" controls>\n",
    "  <source src=\"{final_output}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if Baby is Crying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use DATA_ROOT from config (set earlier)\n",
    "# DATA_ROOT = \"/kaggle/input/listening-beyond-the-cry/Final Dataset\"\n",
    "\n",
    "print('DATA_ROOT in use:', DATA_ROOT)\n",
    "\n",
    "cry_paths = glob.glob(os.path.join(DATA_ROOT, \"Cry\", \"**\", \"*.wav\"), recursive=True)\n",
    "notcry_paths = glob.glob(os.path.join(DATA_ROOT, \"Not-cry\", \"**\", \"*.wav\"), recursive=True)\n",
    "\n",
    "print(\"Cry:\", len(cry_paths))\n",
    "print(\"Not Cry:\", len(notcry_paths))\n",
    "\n",
    "# balance cry = notcry\n",
    "num_notcry = len(notcry_paths)\n",
    "if num_notcry == 0:\n",
    "    raise RuntimeError(f\"No 'Not-cry' audio files found under DATA_ROOT={DATA_ROOT}. Update DATA_ROOT in config.py or the notebook.\")\n",
    "cry_paths_balanced = random.sample(cry_paths, num_notcry)\n",
    "\n",
    "all_files = [(p, 1) for p in cry_paths_balanced] + [(p, 0) for p in notcry_paths]\n",
    "random.shuffle(all_files)\n",
    "\n",
    "print(\"Balanced total:\", len(all_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BabyCryDataset(Dataset):\n",
    "    def __init__(self, file_label_pairs, fixed_frames=224):\n",
    "        self.data = file_label_pairs\n",
    "        self.fixed_frames = fixed_frames\n",
    "\n",
    "        self.mel_tf = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_fft=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath, label = self.data[idx]\n",
    "\n",
    "        # Load audio\n",
    "        wav, sr = torchaudio.load(filepath)\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Mel Spectrogram\n",
    "        mel = self.mel_tf(wav)  # shape = [1, 64, T]\n",
    "\n",
    "        # Normalize\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "\n",
    "        # === FORCE FIXED SHAPE ===\n",
    "        # Interpolate along time dimension to exactly fixed_frames\n",
    "        mel = F.interpolate(mel.unsqueeze(0), size=(64, self.fixed_frames), mode='bilinear', align_corners=False)\n",
    "        mel = mel.squeeze(0)\n",
    "\n",
    "        return mel, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = BabyCryDataset(all_files)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CryClassifier().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for mel, y in train_loader:\n",
    "        mel, y = mel.to(device), y.to(device)\n",
    "\n",
    "        pred = model(mel)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel, y in val_loader:\n",
    "        mel = mel.to(device)\n",
    "        preds = model(mel)\n",
    "\n",
    "        preds = (preds.cpu().numpy() > 0.5).astype(int)\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y.numpy().astype(int))\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_wav(path):\n",
    "    model.eval()\n",
    "\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "    mel_tf = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000, n_fft=1024, hop_length=512, n_mels=64\n",
    "    )\n",
    "    mel = mel_tf(wav)\n",
    "    mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "\n",
    "    mel = mel.unsqueeze(0).to(device)   # batch=1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(mel).item()\n",
    "\n",
    "    if pred > 0.5:\n",
    "        return f\"Cry ({pred:.3f})\"\n",
    "    else:\n",
    "        return f\"Laugh ({pred:.3f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_path = \"notcry_cry_classifier.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(\"Model saved to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#Confusion Matrix Heatmap\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Not Cry\", \"Cry\"],\n",
    "            yticklabels=[\"Not Cry\", \"Cry\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "#Bar Chart of Metrics\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "values = [acc, prec, rec, f1]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(metrics, values)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "#Random Mel Spectrogram Samples from Validation set\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "\n",
    "print(\"\\nShowing 3 random validation samples:\")\n",
    "for _ in range(3):\n",
    "    mel, label = random.choice(val_ds)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.imshow(mel.squeeze().numpy(), aspect='auto', origin='lower')\n",
    "    plt.title(f\"Label: {'Cry' if label==1 else 'Not Cry'}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "#Prediction Distribution Histogram\n",
    "\n",
    "all_probs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for mel, label in val_loader:\n",
    "        mel = mel.to(device)\n",
    "        probs = model(mel).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(all_probs, bins=30)\n",
    "plt.title(\"Prediction Probability Distribution\")\n",
    "plt.xlabel(\"Predicted Probability (Cry)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(predict_wav(\"/kaggle/input/test-crying/dunstan-s-baby-language-the-5-cries_Sfxjiev4.wav\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Different Types of Cries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install audiomentations==0.27.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, random, math, warnings, sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np, pandas as pd\n",
    "import librosa, librosa.display\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DATA_DIR / DATA_ROOT use config-backed values\n",
    "DATA_DIR = getattr(cfg, 'DATA_DIR', DATA_ROOT)\n",
    "\n",
    "DATA_DIR = DATA_DIR  # dataset root\n",
    "SR = 16000\n",
    "DURATION = 7.0\n",
    "N_MELS = 128\n",
    "MFCC_N = 40\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "K_FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print('Using DATA_DIR:', DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_ext = {\".wav\", \".mp3\", \".flac\", \".ogg\"}\n",
    "rows = []\n",
    "\n",
    "root = Path(DATA_DIR)\n",
    "\n",
    "if not root.exists():\n",
    "    print(\"ERROR: DATA_DIR does not exist:\", DATA_DIR)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Iterate over folders\n",
    "for label_dir in root.iterdir():\n",
    "    if label_dir.is_dir():\n",
    "        label = label_dir.name.strip().lower()\n",
    "        for f in label_dir.iterdir():\n",
    "            if f.suffix.lower() in audio_ext:\n",
    "                rows.append({\"path\": str(f), \"label\": label})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"No audio files found. Check DATA_DIR.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmenter = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.4),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "    Shift(min_fraction=-0.1, max_fraction=0.1, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio(path, sr=SR, duration=DURATION):\n",
    "    y, orig_sr = sf.read(path)\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    if orig_sr != sr:\n",
    "        y = librosa.resample(y.astype(np.float32), orig_sr, sr)\n",
    "\n",
    "    target_len = int(sr * duration)\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_log_mel(y, sr=SR):\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return log_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_mfcc_features(y, sr=SR):\n",
    "    mf = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=MFCC_N)\n",
    "    return np.concatenate([mf.mean(axis=1), mf.std(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InfantCryDataset(Dataset):\n",
    "    def __init__(self, df, augment=False, spec_augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "        self.spec_augment = spec_augment\n",
    "\n",
    "        labels = sorted(self.df.label.unique())\n",
    "        self.label2idx = {l:i for i,l in enumerate(labels)}\n",
    "        self.idx2label = {i:l for l,i in self.label2idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        y = load_audio(row.path)\n",
    "\n",
    "        if self.augment:\n",
    "            y = augmenter(samples=y, sample_rate=SR)\n",
    "\n",
    "        log_mel = compute_log_mel(y)\n",
    "        mfcc_feat = compute_mfcc_features(y)\n",
    "\n",
    "        # normalize\n",
    "        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-9)\n",
    "\n",
    "        mel_tensor = torch.tensor(log_mel, dtype=torch.float).unsqueeze(0)\n",
    "        mfcc_tensor = torch.tensor(mfcc_feat, dtype=torch.float)\n",
    "\n",
    "        label_idx = self.label2idx[row.label]\n",
    "        return mel_tensor, mfcc_tensor, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNetMel(nn.Module):\n",
    "    def __init__(self, pretrained=True, out_dim=512):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=pretrained)\n",
    "        base.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.base = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.embedding_dim = base.fc.in_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim=512, mfcc_dim=MFCC_N*2, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim + mfcc_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, emb, mfcc):\n",
    "        return self.fc(torch.cat([emb, mfcc], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(cnn, fusion, optimizer, dl, criterion):\n",
    "    cnn.train(); fusion.train()\n",
    "    total = correct = loss_sum = 0\n",
    "\n",
    "    for mel, mfcc, labels in dl:\n",
    "        mel, mfcc, labels = mel.to(DEVICE), mfcc.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        emb = cnn(mel)\n",
    "        logits = fusion(emb, mfcc)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return loss_sum/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval_model(cnn, fusion, dl, criterion):\n",
    "    cnn.eval(); fusion.eval()\n",
    "    preds_list, label_list = [], []\n",
    "    loss_sum = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel, mfcc, labels in dl:\n",
    "            mel, mfcc, labels = mel.to(DEVICE), mfcc.to(DEVICE), labels.to(DEVICE)\n",
    "            emb = cnn(mel)\n",
    "            logits = fusion(emb, mfcc)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss_sum += loss.item()*labels.size(0)\n",
    "            preds_list.append(logits.argmax(1).cpu().numpy())\n",
    "            label_list.append(labels.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds_list)\n",
    "    labels = np.concatenate(label_list)\n",
    "    return loss_sum/len(labels), preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_kfold(df):\n",
    "    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "    X, y = df['path'].values, df['label'].values\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold, (train_i, val_i) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold}/{K_FOLDS} ---\")\n",
    "\n",
    "        df_train = df.iloc[train_i]\n",
    "        df_val   = df.iloc[val_i]\n",
    "\n",
    "        ds_train = InfantCryDataset(df_train, augment=True)\n",
    "        ds_val   = InfantCryDataset(df_val, augment=False)\n",
    "\n",
    "        # balanced sampling\n",
    "        counts = df_train.label.value_counts().to_dict()\n",
    "        weights = df_train.label.map(lambda x: 1/counts[x]).values\n",
    "        sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "        dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4)\n",
    "        dl_val   = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "        num_classes = len(ds_train.label2idx)\n",
    "\n",
    "        cnn = ResNetMel(pretrained=True).to(DEVICE)\n",
    "        fusion = FusionClassifier(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "        # class weights\n",
    "        class_counts = np.array([counts[l] for l in sorted(counts)])\n",
    "        w = 1/class_counts\n",
    "        w = torch.tensor(w/w.sum()*len(w), dtype=torch.float).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "        optimizer = optim.Adam(list(cnn.parameters()) + list(fusion.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "        best_f1, best_state = -1, None\n",
    "\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            tr_loss, tr_acc = train_one_epoch(cnn, fusion, optimizer, dl_train, criterion)\n",
    "            val_loss, preds, labs = eval_model(cnn, fusion, dl_val, criterion)\n",
    "\n",
    "            rpt = classification_report(labs, preds, output_dict=True)\n",
    "            macro_f1 = rpt[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "            print(f\"Epoch {epoch}: train_acc={tr_acc:.3f} val_f1={macro_f1:.3f}\")\n",
    "\n",
    "            if macro_f1 > best_f1:\n",
    "                best_f1 = macro_f1\n",
    "                best_state = {\n",
    "                    \"cnn\": cnn.state_dict(),\n",
    "                    \"fusion\": fusion.state_dict(),\n",
    "                    \"idx2label\": ds_train.idx2label\n",
    "                }\n",
    "\n",
    "        fold_reports.append(best_state)\n",
    "        print(\"Best Macro F1:\", best_f1)\n",
    "\n",
    "    return fold_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting training...\")\n",
    "    reports = run_kfold(df)\n",
    "    torch.save(reports[-1], \"best_infantcry_model.pth\")\n",
    "    print(\"Saved best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n=== Generating Outputs ===\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. SHOW PER CLASS SAMPLE COUNTS\n",
    "# -----------------------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Number of Samples per Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. SHOW SOME MELSPECTROGRAM IMAGES\n",
    "# -----------------------------------------\n",
    "print(\"\\nShowing mel-spectrograms from few random audio files...\")\n",
    "\n",
    "sample_df = df.sample(3, random_state=42)\n",
    "\n",
    "for i, row in sample_df.iterrows():\n",
    "    y = load_audio(row.path)\n",
    "    mel = compute_log_mel(y)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    librosa.display.specshow(mel, sr=SR, x_axis='time', y_axis='mel')\n",
    "    plt.title(f\"Mel Spectrogram - {row.label}\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. RUN MODEL ON FULL DATA & GET CONF MATRIX\n",
    "# -----------------------------------------\n",
    "print(\"\\nRunning final model for confusion matrix & classification report...\")\n",
    "\n",
    "# load best (last fold) model\n",
    "best = reports[-1]\n",
    "idx2label = best[\"idx2label\"]\n",
    "label2idx = {v:k for k,v in idx2label.items()}\n",
    "\n",
    "num_classes = len(idx2label)\n",
    "\n",
    "cnn = ResNetMel(pretrained=False).to(DEVICE)\n",
    "fusion = FusionClassifier(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "cnn.load_state_dict(best[\"cnn\"])\n",
    "fusion.load_state_dict(best[\"fusion\"])\n",
    "\n",
    "cnn.eval()\n",
    "fusion.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for mel, mfcc, label in DataLoader(\n",
    "        InfantCryDataset(df, augment=False),\n",
    "        batch_size=16, shuffle=False):\n",
    "\n",
    "    mel, mfcc = mel.to(DEVICE), mfcc.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = cnn(mel)\n",
    "        logits = fusion(emb, mfcc)\n",
    "\n",
    "    preds = logits.argmax(1).cpu().numpy()\n",
    "    labels = label.numpy()\n",
    "\n",
    "    all_preds.append(preds)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. CONFUSION MATRIX\n",
    "# -----------------------------------------\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=idx2label.values(),\n",
    "            yticklabels=idx2label.values())\n",
    "plt.title(\"Confusion Matrix (Full Dataset)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5. CLASSIFICATION REPORT\n",
    "# -----------------------------------------\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(idx2label.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SR = 16000\n",
    "DURATION = 7.0\n",
    "N_MELS = 128\n",
    "MFCC_N = 40\n",
    "# MODEL_PATH and TEST_AUDIO should come from config if available\n",
    "MODEL_PATH = MODEL_PATH  # from config import cell\n",
    "TEST_AUDIO = TEST_AUDIO or \"/kaggle/input/baby-cry-sounds/Baby Dataset/tired/249c.wav\"\n",
    "\n",
    "# --------------------------\n",
    "# FEATURE EXTRACTORS\n",
    "# --------------------------\n",
    "def load_audio(path, sr=SR, duration=DURATION):\n",
    "    y, orig_sr = sf.read(path)\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    if orig_sr != sr:\n",
    "        y = librosa.resample(y.astype(np.float32), orig_sr, sr)\n",
    "    target_len = int(sr * duration)\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "    return y\n",
    "\n",
    "def compute_log_mel(y, sr=SR):\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-9)\n",
    "    return log_mel\n",
    "\n",
    "def compute_mfcc_features(y, sr=SR):\n",
    "    mf = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=MFCC_N)\n",
    "    return np.concatenate([mf.mean(axis=1), mf.std(axis=1)])\n",
    "\n",
    "# --------------------------\n",
    "# MODEL DEFINITION (same as training)\n",
    "# --------------------------\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetMel(nn.Module):\n",
    "    def __init__(self, pretrained=True, out_dim=512):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=pretrained)\n",
    "        base.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.base = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.embedding_dim = base.fc.in_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.head(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim=512, mfcc_dim=MFCC_N*2, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim + mfcc_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    def forward(self, emb, mfcc):\n",
    "        return self.fc(torch.cat([emb, mfcc], dim=1))\n",
    "\n",
    "# --------------------------\n",
    "# LOAD MODEL\n",
    "# --------------------------\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "idx2label = checkpoint.get('idx2label', None)\n",
    "if idx2label is None:\n",
    "    # try older key names\n",
    "    idx2label = checkpoint.get('label_map', None) or {i: l for i, l in enumerate(['class0','class1','class2','class3','class4'])}\n",
    "\n",
    "cnn = ResNetMel(pretrained=False, out_dim=512).to(DEVICE)\n",
    "fusion = FusionClassifier(emb_dim=512, mfcc_dim=MFCC_N*2, num_classes=len(idx2label)).to(DEVICE)\n",
    "\n",
    "cnn.load_state_dict(checkpoint['cnn'])\n",
    "fusion.load_state_dict(checkpoint['fusion'])\n",
    "cnn.eval(); fusion.eval()\n",
    "\n",
    "# --------------------------\n",
    "# INFERENCE FUNCTION\n",
    "# --------------------------\n",
    "def predict(audio_path):\n",
    "    y = load_audio(audio_path)\n",
    "    log_mel = compute_log_mel(y)\n",
    "    mfcc_feat = compute_mfcc_features(y)\n",
    "\n",
    "    mel_tensor = torch.tensor(log_mel, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    mfcc_tensor = torch.tensor(mfcc_feat, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = cnn(mel_tensor)\n",
    "        logits = fusion(emb, mfcc_tensor)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = int(probs.argmax())\n",
    "        pred_label = idx2label.get(pred_idx, str(pred_idx))\n",
    "    return pred_label, probs\n",
    "\n",
    "# --------------------------\n",
    "# RUN ON TEST AUDIO\n",
    "# --------------------------\n",
    "if TEST_AUDIO is None:\n",
    "    print('TEST_AUDIO not set in config; skipping sample predict step.')\n",
    "else:\n",
    "    label, probs = predict(TEST_AUDIO)\n",
    "    print(\"Predicted label:\", label)\n",
    "    print(\"Class probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def predict_tta(audio_path, n_augment=5):\n",
    "    y_orig = load_audio(audio_path)\n",
    "    probs_list = []\n",
    "    for _ in range(n_augment):\n",
    "        y = copy.deepcopy(y_orig)\n",
    "        # apply augmentation\n",
    "        y = augmenter(samples=y, sample_rate=SR)\n",
    "        log_mel = compute_log_mel(y)\n",
    "        mfcc_feat = compute_mfcc_features(y)\n",
    "        mel_tensor = torch.tensor(log_mel, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        mfcc_tensor = torch.tensor(mfcc_feat, dtype=torch.float).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = cnn(mel_tensor)\n",
    "            logits = fusion(emb, mfcc_tensor)\n",
    "            probs_list.append(torch.softmax(logits, dim=1).cpu().numpy()[0])\n",
    "    avg_probs = np.mean(probs_list, axis=0)\n",
    "    pred_idx = avg_probs.argmax()\n",
    "    pred_label = idx2label[pred_idx]\n",
    "    return pred_label, avg_probs\n",
    "\n",
    "label, probs = predict_tta(\"/kaggle/input/test-hunger/dunstan-s-baby-language-the-5-cries_Sfxjiev4.wav\")\n",
    "print(\"Predicted label (TTA):\", label)\n",
    "print(\"Class probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2758254,
     "sourceId": 5215181,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4195468,
     "sourceId": 7243196,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6912059,
     "sourceId": 11089116,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8676384,
     "sourceId": 13648235,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8694423,
     "sourceId": 13673722,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8820546,
     "sourceId": 13847860,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
